{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NzyqehHjUlG"
   },
   "source": [
    "# ML in Cybersecurity: Task II\n",
    "\n",
    "## Team\n",
    "  * **Team name**:  *fill this in*\n",
    "  * **Members**:  *fill this in. format: name1 (email1), name2 (email2), ...*\n",
    "\n",
    "\n",
    "## Logistics\n",
    "  * **Due date**: 25th Nov. 2021, 23:59:59 (email the completed notebook including outputs to mlcysec_ws2022_staff@lists.cispa.saarland)\n",
    "  * Email the completed notebook to mlcysec_ws2022_staff@lists.cispa.saarland \n",
    "  * Complete this in the previously established **teams of 3**\n",
    "  * Feel free to use the course forum to discuss.\n",
    "  \n",
    "  \n",
    "## About this Project\n",
    "In this project, we dive into the vulnerabilities of machine learning models and the difficulties of defending against them. To this end, we ask you to implement an evasion attack (craft adversarial examples) yourselves, and defend your own model.   \n",
    "\n",
    "\n",
    "## A Note on Grading\n",
    "The total number of points in this project is 100. We further provide the number of points achievable with each excercise. You should take particular care to document and visualize your results.\n",
    "\n",
    "Whenever possible, please use tools like tables or figures to compare the different findings\n",
    "\n",
    "\n",
    " \n",
    "## Filling-in the Notebook\n",
    "You'll be submitting this very notebook that is filled-in with (all) your code and analysis. Make sure you submit one that has been previously executed in-order. (So that results/graphs are already visible upon opening it). \n",
    "\n",
    "The notebook you submit **should compile** (or should be self-contained and sufficiently commented). Check tutorial 1 on how to set up the Python3 environment.\n",
    "\n",
    "It is extremely important that you **do not** re-order the existing sections. Apart from that, the code blocks that you need to fill-in are given by:\n",
    "```\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "```\n",
    "Feel free to break this into multiple-cells. It's even better if you interleave explanations and code-blocks so that the entire notebook forms a readable \"story\".\n",
    "\n",
    "\n",
    "## Code of Honor\n",
    "We encourage discussing ideas and concepts with other students to help you learn and better understand the course content. However, the work you submit and present **must be original** and demonstrate your effort in solving the presented problems. **We will not tolerate** blatantly using existing solutions (such as from the internet), improper collaboration (e.g., sharing code or experimental data between groups) and plagiarism. If the honor code is not met, no points will be awarded.\n",
    "\n",
    " \n",
    "  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ewNwfFvbFaR"
   },
   "outputs": [],
   "source": [
    "import time \n",
    " \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import json \n",
    "import time \n",
    "import pickle \n",
    "import sys \n",
    "import csv \n",
    "import os \n",
    "import os.path as osp \n",
    "import shutil \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    " \n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots \n",
    "plt.rcParams['image.interpolation'] = 'nearest' \n",
    "plt.rcParams['image.cmap'] = 'gray' \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "640GrzbOevr0"
   },
   "outputs": [],
   "source": [
    "# Some suggestions of our libraries that might be helpful for this project\n",
    "from collections import Counter          # an even easier way to count\n",
    "from multiprocessing import Pool         # for multiprocessing\n",
    "from tqdm import tqdm                    # fancy progress bars\n",
    "\n",
    "# Load other libraries here.\n",
    "# Keep it minimal! We should be easily able to reproduce your code.\n",
    "# We only support sklearn and pytorch.\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from sklearn import metrics\n",
    "# We preload pytorch as an example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJZPEAWYMhYB"
   },
   "outputs": [],
   "source": [
    "compute_mode = 'cpu'\n",
    "\n",
    "if compute_mode == 'cpu':\n",
    "    device = torch.device('cpu')\n",
    "elif compute_mode == 'gpu':\n",
    "    # If you are using pytorch on the GPU cluster, you have to manually specify which GPU device to use\n",
    "    # It is extremely important that you *do not* spawn multi-GPU jobs.\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'    # Set device ID here\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    raise ValueError('Unrecognized compute mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxi-lLD0mKHD"
   },
   "source": [
    "#### Helpers\n",
    "\n",
    "In case you choose to have some methods you plan to reuse during the notebook, define them here. This will avoid clutter and keep rest of the notebook succinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBbigqdEmKd8"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1pcmKkyjT7y"
   },
   "source": [
    "# 1. Attacking an ML-model (30 points) \n",
    "\n",
    "In this section, we implement an attack ourselves. First, however, you need a model you can attack. Feel free to choose the DNN/ConvNN from task 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QaJv_d_Dp7OM"
   },
   "source": [
    "## 1.1: Setting up the model and data (4 Points)\n",
    "\n",
    "Load the MNIST data, as done in task 1. \n",
    "\n",
    "Re-use the model from task 1 here and train it until it achieves reasonable accuracy (>92%).\n",
    "\n",
    "If you have the saved checkpoint from task 1, you can load it directly. But please compute here the test accuracy using this checkpoint.  \n",
    "\n",
    "**Hint:** In order to save computation time for the rest of exercise, you might consider having a relatively small model here.\n",
    "\n",
    "**Hint**: You might want to save the trained model to save time later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c688qdGtO1v-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:12<00:00, 780.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       980\n",
      "           1       0.99      0.98      0.99      1135\n",
      "           2       0.95      0.99      0.97      1032\n",
      "           3       0.96      0.98      0.97      1010\n",
      "           4       0.99      0.98      0.98       982\n",
      "           5       0.95      0.98      0.97       892\n",
      "           6       0.99      0.98      0.98       958\n",
      "           7       0.98      0.97      0.98      1028\n",
      "           8       0.98      0.97      0.97       974\n",
      "           9       0.99      0.96      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#We will load a CNN checkpoint\n",
    "\n",
    "# (1)load data \n",
    "batch_size = 8\n",
    "device = \"cpu\"#torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "trainset = datasets.MNIST(root='./data', train=True, transform = transforms.ToTensor(), download=True)\n",
    "testset = datasets.MNIST(root='./data', train=False, transform = transforms.ToTensor(), download=True)\n",
    "x_trainval, x_test, y_trainval, y_test = trainset.data, testset.data, trainset.targets, testset.targets\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('zero', 'one', 'two', 'three',\n",
    "           'four', 'five', 'six', 'seven', 'eight', 'nine')\n",
    "\n",
    "# (2)define model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)        \n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "model = Net().to(device)\n",
    "# (3)define loss, optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# (4)train (You can re-use the trained model from project1)\n",
    "\n",
    "model.load_state_dict(torch.load('my_nn.pt'))\n",
    "\n",
    "\n",
    "# (5)evaluate\n",
    "model.eval()\n",
    "def test():\n",
    "    net = model\n",
    "\n",
    "    preds_nn = []\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            #images, labels = data\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = predicted.numpy() #Fixed datatype error\n",
    "            preds_nn.append(predicted)\n",
    "\n",
    "    return preds_nn\n",
    "pred=test()  \n",
    "print (\"Classification Report\")\n",
    "print(metrics.classification_report(y_test, pred))\n",
    "#print('Epoch %d, Train acc: %f, Test acc: %f' % (epoch, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEQrdyLHsUIu"
   },
   "source": [
    "## 1.2: Implementing the FGSM attack (7 Points)\n",
    "\n",
    "We now want to attack the model trained in the previous step. We will start with the FGSM attack as a simple example. \n",
    "\n",
    "Please implement the FGSM attack mentioned in the lecture. \n",
    "\n",
    "More details: https://arxiv.org/pdf/1412.6572.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcVZnUNbRKOz"
   },
   "outputs": [],
   "source": [
    "#FSGM\n",
    "def FSGM(X, perturb, grad):\n",
    "  sign = grad.sign()\n",
    "  X_adj = X + (perturb*sign)\n",
    "  X_adj = torch.clamp(X_adj, 0 , 1)\n",
    "  return X_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNpI3oUoO1wE"
   },
   "source": [
    "## 1.3: Adversarial sample set (7 Points)\n",
    "\n",
    "* Please generate a dataset containing at least 1,000 adversarial examples using FGSM.\n",
    "\n",
    "* Please vary the perturbation budget (3 variants) and generate 1,000 adversarial examples for each. \n",
    "    * **Hint**: you can choose epsilons within, e.g., = [.05, .1, .15, .2, .25, .3],  using MNIST pixel values in the interval       [0, 1]\n",
    "\n",
    "* Compute the accuracy of each attack set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvYpo9p2O1wF"
   },
   "outputs": [],
   "source": [
    "def test( model, device, test_loader, epsilon ):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    adv_labels = []\n",
    "    adv_data = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = FSGM(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 10: #NEEEED MOOOOREEEE\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "                \n",
    "                adv_labels.append(target.numpy())\n",
    "                adv_data.append(adv_ex)\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples, adv_labels, adv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilons = [.05, .1, .15, .2, .25, .3]\n",
    "epsilons = [.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGkp0B0PO1wJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.15\tTest Accuracy = 6608 / 10000 = 0.6608\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "adv_data = []\n",
    "labels = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex, lab, dt = test(model, device, testloader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)\n",
    "    labels.append(lab)\n",
    "    adv_data.append(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ex3qQp3JolD1"
   },
   "source": [
    "## 1.4: Visualizing the results (7 Points)\n",
    "\n",
    "* Please chose one sample for each class (for example the first when iterating the test data) and plot the (ten) adversarial examples as well as the predicted label (before and after the attack)\n",
    "\n",
    "* Please repeat the visualization for the three sets you have created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x720 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot several examples of adversarial samples at each epsilon\n",
    "cnt = 0\n",
    "draw_in_row = 5\n",
    "plt.figure(figsize=(8,10))\n",
    "for i in range(len(epsilons)):\n",
    "    for j in range(draw_in_row):\n",
    "        cnt += 1\n",
    "        plt.subplot(len(epsilons),len(examples[0]),cnt)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        if j == 0:\n",
    "            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n",
    "        orig,adv,ex = examples[i][j]\n",
    "        plt.title(\"{} -> {}\".format(orig, adv))\n",
    "        plt.imshow(ex, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5: Analyzing the results (5 Points)\n",
    "\n",
    "Please write a brief summary of your findings.  \n",
    "\n",
    "* Does the attack always succeed (the model makes wrong prediction on the adversarial sample)? What is the relationship between the attack success rate and the perturbation budget?\n",
    "* How about the computation cost of the attack? (you can report the time in second) \n",
    "* Does the attack require white-box access to the model?\n",
    "* Feel free to report your results via tables or figures, and mention any other interesting observations \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .35, step=0.05))\n",
    "plt.title(\"Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJUmrv5Bymij"
   },
   "source": [
    "# 2. Defending an ML model (35 points) \n",
    "\n",
    "So far, we have focused on attacking an ML model. In this section, we want you to defend your model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gHUFK6Mymik"
   },
   "source": [
    "## 2.1: Implementing the adversarial training defense (20 Points)\n",
    "\n",
    "* We would like to ask you to implement the adversarial training defense (https://arxiv.org/pdf/1412.6572.pdf) mentioned in the lecture. \n",
    "\n",
    "* You can use the **FGSM adversarial training** method (i.e., train on FGSM examples). \n",
    "\n",
    "* You can also check the adversarial training implementation in other papers, e.g., http://proceedings.mlr.press/v97/pang19a/pang19a.pdf \n",
    "\n",
    "* Choose a certain **maximum perturbation budget** during training that is in the middle of the range you have experimented with before. \n",
    "\n",
    "* We do not require the defense to work perfectly - but what we want you to understand is why it works or why it does not work.\n",
    "\n",
    "**Hint:** You can save the checkpoint of the defended model as we would need it to for the third part of this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DD0UalSeymim"
   },
   "outputs": [],
   "source": [
    "class adversarial_mnist(Dataset):\n",
    "    def __init__(self, data, labels, transform):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx], int(self.labels[idx])\n",
    "        img = Image.fromarray(img, mode='L')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_dataset = adversarial_mnist(adv_data[0], labels[0], transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7412, 0.7451,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5608, 0.9686, 0.6000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9686, 0.9490, 0.3373,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.7529, 0.9882, 0.7333, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.2431, 0.7255, 0.0706, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.3490, 0.9255, 0.8510, 0.1843, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.8471, 0.9922, 0.2353, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.8314, 1.0000, 0.3176, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.8078, 0.9882, 0.2667, 0.0000,\n",
       "           0.0000, 0.0000, 0.1882, 0.9490, 0.9922, 0.3490, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5137, 0.9843, 0.8314, 0.0824, 0.0000,\n",
       "           0.0000, 0.0431, 0.6549, 0.9882, 0.7725, 0.0196, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1137, 0.9098, 0.9686, 0.2471, 0.0000, 0.0000,\n",
       "           0.0000, 0.6000, 0.9882, 0.8863, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1765, 0.8588, 0.9882, 0.5608, 0.0000, 0.0000, 0.0000,\n",
       "           0.4549, 0.9765, 0.9882, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157,\n",
       "           0.3765, 0.9922, 1.0000, 0.9922, 0.7843, 0.4784, 0.0275, 0.0980,\n",
       "           0.7882, 0.9804, 0.6196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3608,\n",
       "           0.9882, 0.9882, 0.9922, 0.8510, 0.9882, 0.9882, 0.7843, 0.8902,\n",
       "           0.9882, 0.9059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3412, 0.9843,\n",
       "           0.9686, 0.9059, 0.2549, 0.1882, 0.7412, 0.9882, 0.9882, 0.9922,\n",
       "           0.9882, 0.9843, 0.8902, 0.1373, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7451, 0.8667,\n",
       "           0.3843, 0.0000, 0.0000, 0.0000, 0.1647, 0.7686, 0.9882, 0.9922,\n",
       "           0.9882, 0.9882, 0.6353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4353, 0.1137,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.2431, 0.9373, 0.9882, 0.3373,\n",
       "           0.1647, 0.1647, 0.0549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0588, 0.5804, 0.9922, 0.8549, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.4745, 0.9882, 0.9059, 0.1098, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.1216, 0.8667, 0.9843, 0.5059, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.8549, 0.9882, 0.6275, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.4784, 0.9882, 0.3216, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.__getitem__(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.7373, 0.7373, 0.2353, 0.2392],\n",
       "          [0.0706, 0.0667, 0.3176, 0.2471, 0.0000, 0.0000, 0.5020, 0.2471,\n",
       "           0.2667, 0.2627, 0.7647, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.3529, 0.3490, 0.0980, 0.2471,\n",
       "           0.5647, 0.5608, 0.3098, 0.2471],\n",
       "          [0.0000, 0.0000, 0.5020, 0.2471, 0.0000, 0.0000, 0.5020, 0.2471,\n",
       "           0.0863, 0.0863, 0.3373, 0.2471, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9961, 0.2392,\n",
       "           0.5490, 0.5490, 0.2980, 0.2471, 0.5961, 0.5961, 0.3451, 0.2471,\n",
       "           0.5255, 0.5216, 0.2706, 0.2471],\n",
       "          [0.5176, 0.5137, 0.2627, 0.2471, 0.5961, 0.5961, 0.3451, 0.2471,\n",
       "           0.1176, 0.1137, 0.3647, 0.2471, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431],\n",
       "          [0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.6039, 0.6000, 0.0980, 0.2431, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.6039, 0.6000, 0.0980, 0.2431,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_dataset.__getitem__(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(adv_dataset, batch_size=2, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader, 0):\n",
    "    print(len(data))\n",
    "    inputs, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            #inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 20 == 19:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 20))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training') \n",
    "     \n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Evaluation (10 Points)\n",
    "\n",
    "* Craft adversarial examples using the **defended** model. This entails at least 1,000 examples crafted via FGSM. \n",
    "    * Create one set using a budget that is **less than (within)** the one used in training.\n",
    "    * Create another set using a budget that is **higher than** the one used in training. \n",
    "    * You can use two values of epsilons from question 1.3 \n",
    "    \n",
    "* Evaluate the **defended** model on these two adversarial examples sets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "print('Accuracy on the lower-budget adversarial samples (FGSM) %.2f'%acc_FGSM1)\n",
    "print('Accuracy on the lower-budget adversarial samples (FGSM) after defense %.2f'%acc_FGSM_defend1)\n",
    "\n",
    "print('Accuracy on the higher-budget adversarial samples (FGSM) %.2f'%acc_FGSM2)\n",
    "print('Accuracy on the higher-budget adversarial samples (FGSM) after defense %.2f'%acc_FGSM_defend2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Discussion (5 points)\n",
    "\n",
    "* How successful was the defense against the attack compared to the undefended model? How do you interpret the difference?\n",
    "* How did the two sets differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: I-FGSM attack (35 points) \n",
    "\n",
    "* FGSM is one of the simplest and earliest attacks. Since then, many more advanced attacks have been proposed. \n",
    "* One of them is the Iterative-FGSM (https://arxiv.org/pdf/1607.02533.pdf), where the attack is repeated multiple times.\n",
    "* In this part, we ask you to please implement the iterative FGSM attack. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Implementing the I-FGSM attack (10 Points)\n",
    "\n",
    "**Hints**: \n",
    "\n",
    "* Your code should have an attack loop. At each step, the FGSM attack that you have implemented before is computed using a small step.\n",
    "* After each step, you should perform a per-pixel clipping to make sure the image is in the allowed range, and that the perturbation is within budget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Attack the undefended model (5 Points)\n",
    "\n",
    "* We will first attack the **undefended model** (i.e., without adversarial training).\n",
    "\n",
    "* Choose one perturbation budget from Question **1.3** for comparison. \n",
    "\n",
    "    * Hint: A simple way to choose the small step is to divide the total budget by the number of steps (e.g., 10).\n",
    "\n",
    "* Please generate 1000 adversarial examples using the **undefended** model and the **I-FGSM** you implemented. \n",
    "\n",
    "* Please compute the accuracy of the adversarial set on the **undefended** model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1: Findings and comparison with FGSM (8 points)\n",
    "\n",
    "* Please report your findings. How successful was the attack? \n",
    "\n",
    "* What do you expect when increasing the number of steps? (you can experiment with different parameters of the attack and report your findings) \n",
    "\n",
    "* Compare with the basic FGSM. Using the same perturbation budget and using the same model, which attack is more successful? Why do you think this is the case? What about the computation time?\n",
    "\n",
    "* Feel free to report any interesting observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Attack the defended model (5 poinst) \n",
    "\n",
    "* In the previous question, we attacked the **undefended model**. \n",
    "\n",
    "* Now, we want to explore how successful the previous implemented defense (FGSM adversarial training) is againts this new attack. (we will not implement a new defense here, we will be reusing your previous checkpoint of the **defended model**)\n",
    "\n",
    "\n",
    "* Use the **defended model** to create one set of adversarial examples. Use a perturbation budget from Question **2.2** for comparison.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1: Discussion (7 points) \n",
    "* Please report your results. How successful was the attack on the defended model? \n",
    "* Compare it with the success of the FGSM attack on the defended model. What do you observe? How do you interpret the difference? \n",
    "* How do you think you can improve the defense against I-FGSM attack?\n",
    "\n",
    "\n",
    "* Feel free to state any interesting findings you encountered during this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers go here**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_3_Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
